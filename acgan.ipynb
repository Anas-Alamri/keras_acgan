{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network (GAN)\n",
    "\n",
    "A generative adversarial network (GAN) consists of two neural networks trained in opposition to one another.\n",
    "\n",
    "The **generator G** takes as input a random noise vector z and outputs an image $X_{fake} = G(z)$.\n",
    "\n",
    "The **discriminator D** receives as input either a training image or a synthesized image from the generator and outputs a probability distribution $P(S \\mid X) = D(X)$ over possible image sources.\n",
    "\n",
    "The discriminator is trained to **maximize** the log-likelihood it assigns to the correct source:\n",
    "\n",
    "$$\n",
    "L = E \\left [log \\ P(S=real \\mid X_{real}) \\right ] +  E \\left [log \\ P(S=fake \\mid X_{fake}) \\right ]\n",
    "$$\n",
    "\n",
    "**Note:** This is the same as cross-entropy, we are only interested in maximizing the log probability of the correct labels.\n",
    "\n",
    "The generator is trained to **minimize** the second term in the expression:\n",
    "\n",
    "$$\n",
    "L = E \\left [log \\ P(S=fake \\mid X_{fake}) \\right ]\n",
    "$$\n",
    "\n",
    "I had some difficulties to understand this training objective. The generator is not trained in isolation, instead the expression trained is $D(G(z))$ where the weights of the discriminator are fixed. The goal is to come up with a generator that produces images that are so similar to the real images that the discriminator can not classify them as fake any more. Therefore the optimization goal of the generator makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary classifier GAN (AC-GAN)\n",
    "\n",
    "Every generated sample has a corresponding class label, $c âˆ¼ p_c$ in addition to the noise z. \n",
    "\n",
    "The generator uses both to generate images $X_{fake} = G(c, z)$.\n",
    "\n",
    "The discriminator returns both a probability distribution over sources and a probability distribution over class labels: $P(S \\mid X), P(C \\mid X) = D(X)$.\n",
    "\n",
    "The objective function has two parts: \n",
    " * the log-likelihood of the correct source $L_S$\n",
    " * the log-likelihood of the correct class $L_C$\n",
    " \n",
    "$ L_S = E \\left [log \\ P(S=real \\mid X_{real}) \\right ] +  E \\left [log \\ P(S=fake \\mid X_{fake}) \\right ] $\n",
    "\n",
    "$ L_C = E \\left [log \\ P(C=c \\mid X_{real}) \\right ] +  E \\left [log \\ P(C=c \\mid X_{fake}) \\right ] $\n",
    "\n",
    "D is trained to maximize $ L_S + L_C $\n",
    "\n",
    "**Note:** the goal is a discriminator that is good in detecting fakes **and** good in predicting class labels\n",
    "\n",
    "G is trained to maximize $ L_C - L_S $\n",
    "\n",
    "**Note:** this expression maximizes $L_C$ and minimizes $L_S$ at the same time. The goal is a generator that produces images that are typical for there class but can not be classified as fakes.\n",
    "\n",
    "**Other Note:** Compared to the description of GANs above the generator optimization goal minimizes the whole $L_S$ instead of just the seconds part $E \\left [log \\ P(S=fake \\mid X_{fake}) \\right ]$. The result is the same because in the context of training the generator all images are fake. So there is only the second expression of $L_S$.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
